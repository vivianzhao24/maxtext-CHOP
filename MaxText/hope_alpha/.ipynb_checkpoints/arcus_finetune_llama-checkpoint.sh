#!/bin/bash


# Example Usage: bash finetune_llama3_8b_multihost.sh FUNCTION=train FINETUNE_RUN_NAME=finetune_llama3-8b_9-12-24


set -ex

# Unset envs that should be set anew each time script is run 
unset FUNCTION
unset FINETUNE_RUN_NAME
unset BASE_OUTPUT_PATH

# Set environment variables from CLI arguments passed to script
for ARGUMENT in "$@"; do
    IFS='=' read -r KEY VALUE <<< "$ARGUMENT"
    export "$KEY"="$VALUE"
    echo "$KEY"="$VALUE"
done


export MODEL_VARIATION='llama3.1-8b'

if [ -z "${BASE_OUTPUT_PATH}" ]; then
    # Non-Googlers please remember to point `BASE_OUTPUT_PATH` to a GCS bucket that you own, this bucket will store all the files generated by MaxText during a run
    # Use the same BASE_OUTPUT_PATH as end_to_end/tpu/llama3/8b/1_test_llama3_8b.sh
    export BASE_OUTPUT_PATH=${GSEAST}
    echo "BASE_OUTPUT_PATH is not set, using BASE_OUTPUT_PATH = ${BASE_OUTPUT_PATH}"
fi

if [ -z "${FUNCTION}" ] || [ -z "${FINETUNE_RUN_NAME}" ]; then
  echo "Error: FUNCTION and/or FINETUNE_RUN_NAME have not been passed to script"
  exit 1
fi

# Non-Googlers please remember to point `DATASET_PATH` to the GCS bucket where you have your training data
export DATASET_NAME=<path to tfds dataset>
export BASE_OUTPUT_PATH=<output data path>
# We define `CONVERTED_CHECKPOINT` to refer to the checkpoint subdirectory. This way it is easier to use this path in the `train.py` and `decode.py` commands
export CONVERTED_CHECKPOINT=${BASE_OUTPUT_PATH}/maxtext-${MODEL_VARIATION}-instruct/scanned_chkpt/0/items


train(){
# Note the escaped $ in PATH to prevent expansion in local shell as opposed to expansion on host shell. 
# Remote host will receive as one command since the backslashes are evaluated on local 
# xla flag for v6e LIBTPU_INIT_ARGS='--xla_tpu_scoped_vmem_limit_kib=98304'
# xla flag for v5e JAX_FORCE_TPU_INIT=true LIBTPU_INIT_ARGS='--xla_tpu_use_minor_sharding_for_major_trivial_input=true --xla_tpu_relayout_group_size_threshold_for_reduce_scatter=1 --xla_tpu_scoped_vmem_limit_kib=98304 --xla_tpu_enable_data_parallel_all_reduce_opt=true --xla_tpu_data_parallel_opt_different_sized_ops=true --xla_tpu_enable_async_collective_fusion=true --xla_tpu_enable_async_collective_fusion_fuse_all_gather=true --xla_tpu_enable_async_collective_fusion_multiple_steps=true --xla_tpu_overlap_compute_collective_tc=true --xla_enable_async_all_gather=true' 
cd ~/maxtext
python3 multihost_runner.py --TPU_PREFIX=$TPU_NAME --COMMAND="export PATH=\"/home/sa_112413943341569390945/.local/bin:\$PATH\"; \
JAX_FORCE_TPU_INIT=true  \
python MaxText/train.py MaxText/configs/base.yml \
base_output_directory=${BASE_OUTPUT_PATH} \
dataset_path=${DATASET_NAME} \
tokenizer_path=assets/tokenizer_llama3.tiktoken \
load_full_state_path=${CONVERTED_CHECKPOINT} \
per_device_batch_size=2 \
checkpoint_period=5 \
allow_split_physical_axes=True \
run_name=${FINETUNE_RUN_NAME} \
async_checkpointing=False \
model_name=${MODEL_VARIATION} \
train_data_column='inputs' \
eval_data_column='inputs' \
tokenize_train_data=False \
tokenize_eval_data=False \
add_bos=False \
add_eos=False \
max_target_length=8192 \
tfds_iter_checkpointing=True \
attention='flash' \
steps=100 \
remat_policy=full"
}

remove_checkpoint(){
    gsutil rm -r ${BASE_OUTPUT_PATH}/${FINETUNE_RUN_NAME}
}

# pass function name as an argument
eval "$FUNCTION"
